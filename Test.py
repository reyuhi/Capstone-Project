from __future__ import division
import pandas as pd
import numpy  as  np
train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')
#more about kinetic features  developed  by Daia Alexandru    here  on the next  blog  please  read  last article :
#https://alexandrudaia.quora.com/

##############################################creatinng   kinetic features for  train #####################################################
def  kinetic(row):
    probs=np.unique(row,return_counts=True)[1]/len(row)
    kinetic=np.sum(probs**2)
    return kinetic
    

first_kin_names=[col for  col in train.columns  if '_ind_' in col]
subset_ind=train[first_kin_names]
kinetic_1=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_1.append(k)
second_kin_names= [col for  col in train.columns  if '_car_' in col and col.endswith('cat')]
subset_ind=train[second_kin_names]
kinetic_2=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_2.append(k)
third_kin_names= [col for  col in train.columns  if '_calc_' in col and  not col.endswith('bin')]
subset_ind=train[second_kin_names]
kinetic_3=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_3.append(k)
fd_kin_names= [col for  col in train.columns  if '_calc_' in col and  col.endswith('bin')]
subset_ind=train[fd_kin_names]
kinetic_4=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_4.append(k)
train['kinetic_1']=np.array(kinetic_1)
train['kinetic_2']=np.array(kinetic_2)
train['kinetic_3']=np.array(kinetic_3)
train['kinetic_4']=np.array(kinetic_4)

############################################reatinng   kinetic features for  test###############################################################

first_kin_names=[col for  col in test.columns  if '_ind_' in col]
subset_ind=test[first_kin_names]
kinetic_1=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_1.append(k)
second_kin_names= [col for  col in test.columns  if '_car_' in col and col.endswith('cat')]
subset_ind=test[second_kin_names]
kinetic_2=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_2.append(k)
third_kin_names= [col for  col in test.columns  if '_calc_' in col and  not col.endswith('bin')]
subset_ind=test[second_kin_names]
kinetic_3=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_3.append(k)
fd_kin_names= [col for  col in test.columns  if '_calc_' in col and  col.endswith('bin')]
subset_ind=test[fd_kin_names]
kinetic_4=[]
for row in range(subset_ind.shape[0]):
    row=subset_ind.iloc[row]
    k=kinetic(row)
    kinetic_4.append(k)
test['kinetic_1']=np.array(kinetic_1)
test['kinetic_2']=np.array(kinetic_2)
test['kinetic_3']=np.array(kinetic_3)
test['kinetic_4']=np.array(kinetic_4)
##################################################################end of kinetic features################################################################


sub=pd.read_csv('../input/sample_submission.csv')
trainvec=train[[col for col in train.columns if  col not in ['id','target']]]
y=train['target']
trainvec=np.array(trainvec)
y=np.array(y)
testvec=test[[col for col  in test.columns if   col not in ['id']]]
testvec=np.array(testvec)     
 
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator
from sklearn.base import clone
from sklearn.pipeline import _name_estimators
from sklearn.metrics import r2_score
import pandas as pd 
import  random as random
import numpy as np
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_squared_error
import heapq
from sklearn.linear_model import  LinearRegression,BayesianRidge
from sklearn.ensemble import  RandomForestRegressor,RandomForestClassifier
from sklearn.datasets import load_digits
from sklearn.ensemble import *
import math
 
from sklearn.tree import *
 
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import *
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
 
 
class BadErrorTreshold(Exception):
    pass
 
class EnsembleRegressors(object):
    ''''
    ensembleRegressor is a class of ensembles generated by a  fix number of  iterations.This class is abstract 
    meaning  it  is not possible  to create instances  of it.,,stochasticLearning''  has interface  role. '''
    def __init__(self,regressors,sampling,iterations,test,X,y,k_fold):
        ''''intialize ensemble with  near all  parameters- see that   some of them are  intialized  by other methods'''
        self.sampling=sampling#dropout(cv) ratio-percentage
        if (self.sampling<=0.0 or self.sampling>=1.0):
            raise ValueError('Sampling  must be percent like value between[0,1) ex 0.75, you introduced :',self.sampling)
 
 
        self.test=test#final test set
        self.X=X#original train features
        self.y=y#original train outcome
        self.iterations=iterations#number of  iterations in the  stochastic ensemble
        self.regressors=regressors#ensemble models  for  each iteration      
        self.randomIndexes=[ [] for rounds in range(self.iterations)]#filled in with random dropout  by getStochasticDataSets()
        self.error=[]#error for  each averaged iteration
 
 
        self.stochasticPredictions=[ [] for  rounds in range(self.iterations)]#predictions on random drop outs
        self.finalPredictions=[]
 
        self.x_train=[]#his   next  4 rows are intialized  by getStochasticDataSets() according to random chunks
        self.y_train=[]
        self.x_test=[]
        self.y_test=[]
 
        self.new_x_ts=[]
        self.new_x_tr=[]
        self.new_test_final=[]
        self.k_folds=k_fold
        
    def returnError(self):
        '''returns : error for  each iteration-just like cross-validation'''
        return self.error
    
    def returnRandomIndexes(self):
        '''returns: random  indexes  for  each dropout'''
        return self.randomIndexes
    
    def returnstochasticPredictions(self):
        '''returns:predictions  on random dropous'''
        return self.stochasticPredictions
    
    def getStochasticDataSets(self):
        '''
        For some  potentially  iteration creates a random  idx variable representing the index  for random  dropout.
        Splits the train in  train/test(x_train,y_train,features and outcome for cross val train-x_test,y_test for cross_val test
        .Updated this train-test  variables with corresponding  values.See that  the initial vales are empty in the __init__.
        returns: idx(index of  dropout rows).
        
        '''
        x_train, x_test, y_train, y_test = train_test_split(self.X, self.y, test_size=self.sampling)
        self.x_train=x_train
        self.x_test=x_test
        self.y_train=y_train
        self.y_test=y_test
        #train_i, test_i = train_test_split(np.arange(self.X.shape[0]), train_size = self.sampling)
        #self.x_train=self.X[train_i]
        #self.y_train=self.y[train_i]
        #self.x_test=self.X[test_i]
        #self.y_test=self.y[test_i]
        #idx=test_i
        #return  idx
    
    def training(self,i):
        ''''
        For a  particular iteration ,,i''  calls getStochasticDataSets() method.In this was the randomIndexes feature is
        fiiled with corresponding index dropouts at position ,,i''.Since train/test new data sets according to cross validation
        are   filled  by getStochasticDataSets()  independet  of iteration number , this data sets will update for each new iter.
        performmed in stochasticLearning().'''
        self.getStochasticDataSets()
        #blending- we have x_train,y_train,x_test,y_test   our stochastic  dropouts for diversity
        #we are  going first  to make blend train,blend test from this dropoutsthen will see evaluation on the dropout
        clfs=self.regressors
        n_folds=self.k_folds
        skf=list(StratifiedKFold(self.y_train,n_folds))
        blend_tr=np.zeros((self.x_train.shape[0],len(clfs)))
        blend_ts=np.zeros((self.x_test.shape[0],len(clfs)))#for loca al evaluation
        blend_final_test=np.zeros((self.test.shape[0],len(clfs)))
 
        for  j , clf in  enumerate(clfs):
            print(clf)
            blend_ts_j=np.zeros((self.x_test.shape[0],len(skf)))
            blend_final_j=np.zeros((self.test.shape[0],len(skf)))
            for i ,(tr,ts) in enumerate(skf):
                print("fold",i)
                x_tr=self.x_train[tr]
                y_tr=self.y_train[tr]
                x_ts=self.x_train[ts]
                y_ts=self.y_train[ts]
                clf.fit(x_tr,y_tr)
                y_sub=clf.predict_proba(x_ts)[:,1]
                blend_tr[ts,j]=y_sub
                blend_ts_j[:,i]=clf.predict_proba(self.x_test)[:,1]
                blend_final_j[:,i]=clf.predict_proba(self.test)[:,1]
            blend_ts[:,j]=blend_ts_j.mean(1)
            blend_final_test[:,j]=blend_final_j.mean(1)
        #blend on evaluation
        clf = LogisticRegression()
        clf.fit(blend_tr, self.y_train)
        
        pred_eval=  clf.predict_proba(blend_ts)[:,1]
        self.error.append(log_loss(self.y_test,pred_eval))
        #blend  on final test set:
        final_sub=clf.predict_proba(blend_final_test)[:,1]
        self.finalPredictions.append(final_sub)
        
        
 
        return self
    
    def getFinalPrediction(self,errorTreshold):
       raise NotImplementedError###averages models  according to best top prediction  trehsold  with different implementantions
    
    
        
    
    def stochasticLearning(self):
         raise NotImplementedError#Since EnsemebleRegressors class is abstract this will act  as an interface
                                  #and will be implemented   by  this  EnsembleRegressors sublclasses  for averaging and
                                  #weighted average
    
        
class AveragingModels(EnsembleRegressors):
    '''Inherits everything from EnsembleRegressors  class and implements stochastiLearning method  averaging models 
    for each iteration'''
 
    def stochasticLearning(self):
        
        
        for iter in range(self.iterations):
            print('Stochastic Iteration number ',iter)
            self.training(iter)
 
    def getFinalPrediction(self,errorTreshold):
        if (errorTreshold<=self.iterations and errorTreshold>0):
            
            topErrorIndex=heapq.nsmallest(errorTreshold, range(len(self.error)), self.error.__getitem__)
            finalAvg=0
            for topError in topErrorIndex:
                finalAvg=finalAvg+self.finalPredictions[topError]
            finalAvg=(finalAvg)/float(len(topErrorIndex))
            return finalAvg
        else:
            raise BadErrorTreshold
        
    
from sklearn.cross_validation import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import  xgboost as xgb
"""define classifiers"""
from sklearn.metrics import log_loss
clfs = [RandomForestClassifier(n_estimators=109, n_jobs=-1, criterion='gini'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=7, n_estimators=100),
            RandomForestClassifier(n_estimators=120, n_jobs=-1, criterion='gini'),
            RandomForestClassifier(n_estimators=80, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=120, n_jobs=-1, criterion='gini'),
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            RandomForestClassifier(n_estimators=150, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=7, n_estimators=80),
            ExtraTreesClassifier(n_estimators=70, n_jobs=-1, criterion='gini'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50),
            RandomForestClassifier(n_estimators=170, n_jobs=-1, criterion='gini'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=120),
            RandomForestClassifier(n_estimators=180, n_jobs=-1, criterion='entropy'),
            RandomForestClassifier(n_estimators=200, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=120, n_jobs=-1, criterion='gini')
            
            ]

""" run stuff with  0.1 dropout , 3 iterations,predict on testvec, train on trainvec and  y ,do 10
stratified  sampling"""
import  xgboost  as xgb
 
o=AveragingModels([xgb.XGBClassifier(n_estimators=359)],0.9,1,testvec,trainvec,y,35)
o.stochasticLearning()
print(o.error)

prediction=o.getFinalPrediction(1)
sub['target']=prediction
sub.to_csv('ensemble_submissionKinetic.csv', index=False)